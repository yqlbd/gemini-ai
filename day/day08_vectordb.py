import chromadb
from chromadb import Documents, EmbeddingFunction, Embeddings
from sentence_transformers import SentenceTransformer

# 1. åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹
print("ğŸ¤– æ­£åœ¨åŠ è½½æœ¬åœ°è¯­ä¹‰å¼•æ“ (paraphrase-multilingual-MiniLM-L12-v2)...")
local_model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")


class LocalEmbeddingFunction(EmbeddingFunction):
    """
    ä¿®å¤ç‰ˆï¼šç»§æ‰¿å®˜æ–¹åŸºç±»ï¼Œå¹¶å®ç° name() æ–¹æ³•
    """

    def __call__(self, input: Documents) -> Embeddings:
        # encode æ–¹æ³•é»˜è®¤è¿”å› numpy arrayï¼ŒChroma éœ€è¦ list
        embeddings = local_model.encode(input)
        return [e.tolist() for e in embeddings]

    def name(self) -> str:
        # âœ… å…³é”®ä¿®å¤ï¼šChromaDB éœ€è¦è¿™ä¸ªæ–¹æ³•æ¥éªŒè¯æ¨¡å‹èº«ä»½
        return "local_sentence_transformer_v2"


def main():
    print("ğŸ“¦ æ­£åœ¨åˆå§‹åŒ– ChromaDB æ•°æ®åº“ (æœ¬åœ°æŒä¹…åŒ–)...")

    # 2. åˆå§‹åŒ–æŒä¹…åŒ–å®¢æˆ·ç«¯
    client = chromadb.PersistentClient(path="rst/chroma_db")

    # 3. åˆ›å»ºè®°å¿†é›†åˆ
    collection = client.get_or_create_collection(
        name="pangdundun_memory", embedding_function=LocalEmbeddingFunction()
    )

    # 4. å‡†å¤‡ä¸€äº›â€œéç»“æ„åŒ–â€çš„æ—¥è®°æ•°æ®
    documents = [
        "èƒ–å¢©å¢©æœ€çˆ±åƒçš„æ˜¯æ°´ç…®é¸¡èƒ¸è‚‰ï¼Œæ¯æ¬¡çœ‹åˆ°éƒ½ä¼šæµå£æ°´ã€‚",  # doc1
        "èƒ–å¢©å¢©éå¸¸è®¨åŒæ´—æ¾¡ï¼Œæ¯æ¬¡å»å® ç‰©åº—éƒ½è¦èº²åœ¨åºŠåº•ä¸‹ã€‚",  # doc2
        "ä»Šå¤©å¤©æ°”ä¸é”™ï¼Œèµµä¸€æ¸…ï¼ˆå®ï¼‰å¸¦èƒ–å¢©å¢©åœ¨è‰åœ°ä¸Šæ¥åˆ°äº†ä¸‰æ¬¡é£ç›˜ã€‚",  # doc3
        "èƒ–å¢©å¢©çš„ç”Ÿæ—¥æ˜¯2023å¹´5æœˆ20æ—¥ï¼Œæ˜¯ä¸ªé‡‘ç‰›åº§å®å®ï¼Œæ€§æ ¼å¾ˆå€”å¼ºã€‚",  # doc4
    ]

    ids = ["diary_001", "diary_002", "diary_003", "diary_004"]

    metadatas = [
        {"type": "diet", "date": "2026-01-10"},
        {"type": "habit", "date": "2026-01-11"},
        {"type": "activity", "date": "2026-01-12"},
        {"type": "info", "date": "2023-05-20"},
    ]

    print("âœï¸ æ­£åœ¨å†™å…¥è®°å¿†æ•°æ® (Upsert)...")
    collection.upsert(documents=documents, ids=ids, metadatas=metadatas)
    print(f"âœ… æˆåŠŸå­˜å‚¨äº† {collection.count()} æ¡è®°å¿†ï¼")

    # 5. è§è¯å¥‡è¿¹æ—¶åˆ»ï¼šè¯­ä¹‰æœç´¢
    print("\nğŸ” å¼€å§‹æ„å›¾åŒ¹é…æµ‹è¯• (RAG çš„æ ¸å¿ƒ)ï¼š")
    print("-" * 50)

    test_queries = [
        "ç‹—ç‹—å–œæ¬¢åƒä»€ä¹ˆé£Ÿç‰©ï¼Ÿ",
        "å®ƒå®³æ€•ä»€ä¹ˆäº‹æƒ…ï¼Ÿ",
        "è¿™åªç‹—å¤šå¤§äº†ï¼Ÿ",
        "Pythonç¼–ç¨‹è¯­è¨€æ€ä¹ˆå­¦ï¼Ÿ",
    ]

    # è®¾å®šä¸€ä¸ªé˜ˆå€¼ï¼ˆç»è¿‡è§‚å¯Ÿï¼Œ25 ä»¥ä¸‹çš„æ‰æ˜¯é è°±çš„ï¼‰
    # æ³¨æ„ï¼šChroma é»˜è®¤çš„ L2 è·ç¦»æ˜¯è¶Šå°è¶Šå¥½
    DISTANCE_THRESHOLD = 25.0

    for q in test_queries:
        print(f"\nâ“ æé—®: {q}")

        results = collection.query(query_texts=[q], n_results=1)

        if results["documents"] and results["documents"][0]:
            found_doc = results["documents"][0][0]
            distance = results["distances"][0][0]  # è·å–è·ç¦»

            # ğŸ›‘ æ ¸å¿ƒé€»è¾‘ï¼šåŠ äº†è¿™å±‚åˆ¤æ–­ï¼ŒAI å°±ä¸æ•¢ä¹±è¯´äº†
            if distance < DISTANCE_THRESHOLD:
                print(f"ğŸ’¡ æ‰¾åˆ°ç­”æ¡ˆ (è·ç¦» {distance:.4f}):\n   ğŸ‘‰ {found_doc}")
            else:
                print(
                    f"âŒ æœªæ‰¾åˆ°ç›¸å…³ç»“æœ (æœ€è¿‘åŒ¹é…è·ç¦» {distance:.4f} > é˜ˆå€¼ {DISTANCE_THRESHOLD})"
                )
        else:
            print("âŒ åº“é‡Œæ˜¯ç©ºçš„")


if __name__ == "__main__":
    main()
