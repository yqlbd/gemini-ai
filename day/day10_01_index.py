import os
import glob
import chromadb
from chromadb import EmbeddingFunction, Documents, Embeddings
from sentence_transformers import SentenceTransformer


# --- å¤ç”¨ä¹‹å‰çš„é…ç½® (ä¿æŒä¸€è‡´) ---
class LocalEmbeddingFunction(EmbeddingFunction):
    def __init__(self):
        self.model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

    def __call__(self, input: Documents) -> Embeddings:
        embeddings = self.model.encode(input)
        return [e.tolist() for e in embeddings]

    def name(self):
        return "local_sentence_transformer_v2"


# è¿æ¥æ•°æ®åº“
print("ğŸ’¾ è¿æ¥ ChromaDB...")
client = chromadb.PersistentClient(path="rst/chroma_db")
# âš ï¸ æ³¨æ„ï¼šæˆ‘ä»¬å¯ä»¥ç”¨åŒä¸€ä¸ªé›†åˆï¼Œä¹Ÿå¯ä»¥æ–°å»ºä¸€ä¸ªä¸“é—¨å­˜æŠ€æœ¯æ–‡æ¡£çš„
collection = client.get_or_create_collection(
    name="pangdundun_memory",  # è¿™é‡Œæˆ‘ä»¬ç»§ç»­å¾€åŒä¸€ä¸ªè„‘å­é‡Œå¡çŸ¥è¯†
    embedding_function=LocalEmbeddingFunction(),
)


# --- æ ¸å¿ƒé€»è¾‘ 1: æ–‡æœ¬åˆ‡ç‰‡å™¨ (Text Splitter) ---
def split_text(text, chunk_size=300, chunk_overlap=50):
    """
    æ‰‹åŠ¨å®ç°æ»‘åŠ¨çª—å£åˆ‡ç‰‡
    chunk_size: æ¯ä¸ªå—å¤§çº¦å¤šå°‘å­—ç¬¦
    chunk_overlap: é‡å å¤šå°‘å­—ç¬¦ (é˜²æ­¢ä¸Šä¸‹æ–‡åˆ‡æ–­)
    """
    chunks = []
    start = 0
    text_len = len(text)

    while start < text_len:
        end = start + chunk_size
        # æˆªå–ç‰‡æ®µ
        chunk = text[start:end]
        chunks.append(chunk)

        # ç§»åŠ¨çª—å£ (æ­¥é•¿ = å¤§å° - é‡å )
        # è¿™æ ·ä¸‹ä¸€æ¬¡å¾ªç¯å°±ä¼šå›é€€ä¸€ç‚¹ç‚¹ï¼ŒåŒ…å«ä¸Šä¸€å—çš„å°¾å·´
        start += chunk_size - chunk_overlap

    return chunks


# --- æ ¸å¿ƒé€»è¾‘ 2: è¯»å–æ–‡ä»¶å¹¶å¤„ç† ---
def process_markdown_files(directory):
    # æ‰¾åˆ°ç›®å½•ä¸‹æ‰€æœ‰çš„ .md æ–‡ä»¶
    files = glob.glob(os.path.join(directory, "*.md"))
    print(f"ğŸ“‚ å‘ç° {len(files)} ä¸ª Markdown æ–‡ä»¶: {files}")

    total_chunks = 0

    for file_path in files:
        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()

        filename = os.path.basename(file_path)
        print(f"ğŸ”ª æ­£åœ¨åˆ‡åˆ†æ–‡ä»¶: {filename} (é•¿åº¦: {len(content)} å­—ç¬¦)...")

        # 1. åˆ‡ç‰‡
        chunks = split_text(content, chunk_size=500, chunk_overlap=100)

        # 2. å‡†å¤‡å…¥åº“æ•°æ®
        # Chroma éœ€è¦ï¼šIDåˆ—è¡¨, æ–‡æœ¬åˆ—è¡¨, å…ƒæ•°æ®åˆ—è¡¨
        ids = [f"{filename}_chunk_{i}" for i in range(len(chunks))]
        metadatas = [{"source": filename, "chunk_index": i} for i in range(len(chunks))]

        # 3. å…¥åº“ (Upsert)
        print(f"ğŸš€ æ­£åœ¨å°† {len(chunks)} ä¸ªç¢ç‰‡å­˜å…¥å‘é‡åº“...")
        collection.upsert(ids=ids, documents=chunks, metadatas=metadatas)
        total_chunks += len(chunks)

    print(
        f"\nâœ… å…¨éƒ¨å®Œæˆï¼å…±å¤„ç†äº† {len(files)} ä¸ªæ–‡ä»¶ï¼Œå­˜å…¥äº† {total_chunks} æ¡è®°å¿†ç¢ç‰‡ã€‚"
    )


# --- è¿è¡Œ ---
if __name__ == "__main__":
    # å‡è®¾ä½ çš„æ–‡æ¡£éƒ½åœ¨ rst æ–‡ä»¶å¤¹é‡Œ
    # ä½ å¯ä»¥æŠŠ architecture_design.md æ”¾åœ¨è¿™é‡Œé¢æµ‹è¯•
    target_dir = "./rst"

    if not os.path.exists(target_dir):
        os.makedirs(target_dir)
        print(f"âš ï¸ ç›®å½• {target_dir} ä¸å­˜åœ¨ï¼Œå·²è‡ªåŠ¨åˆ›å»ºã€‚è¯·æŠŠ .md æ–‡ä»¶æ”¾è¿›å»å†è¿è¡Œï¼")
    else:
        process_markdown_files(target_dir)
