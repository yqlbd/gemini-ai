# å¯¼å…¥ç³»ç»Ÿå˜é‡ï¼Œä¸»è¦ä¸ºäº†è·å–ä¸åŒç¯å¢ƒAPI Key
from dotenv import load_dotenv

load_dotenv()

# è‡ªå®šä¹‰æœ¬åœ°åµŒå…¥å‡½æ•°ï¼Œä½¿ç”¨ SentenceTransformer
from chromadb import EmbeddingFunction, Documents, Embeddings
from sentence_transformers import SentenceTransformer


class LocalEmbeddingFunction(EmbeddingFunction):
    def __init__(self):

        # æ—¢ç„¶æ˜¯å¤„ç†æ–‡æ¡£ï¼Œæˆ‘ä»¬ç”¨è¿™ä¸€æ¬¾æ”¯æŒå¤šè¯­è¨€çš„æ¨¡å‹ï¼Œæ•ˆæœæ¯”è¾ƒå‡è¡¡
        self.model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

    def __call__(self, input: Documents) -> Embeddings:
        embeddings = self.model.encode(input)
        # åˆ—è¡¨æ¨å¯¼å¼ï¼ŒæŠŠæ¯ä¸ª numpy array è½¬æˆ list
        return [e.tolist() for e in embeddings]

    def name(self):
        return "local_sentence_transformer_v2"


# è¿æ¥æ•°æ®åº“ï¼Œå®šä¹‰å¥½æ•°æ®åº“åœ°å€ï¼Œè¿™é‡Œä¸ºrst/chroma_db
import chromadb
from google import genai
import os
import streamlit as st


@st.cache_resource
def get_chromadb_collection():
    return chromadb.PersistentClient(path="rst/chroma_db").get_collection(
        name="categorized_memory", embedding_function=LocalEmbeddingFunction()
    )


@st.cache_resource
def get_gemini_client():
    return genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))


collection = get_chromadb_collection()
# Gemini API å®¢æˆ·ç«¯
gemini_client = get_gemini_client()

# --- 2. RAG ç³»ç»Ÿä¸»å‡½æ•°ï¼Œç»“åˆæ£€ç´¢å’Œç”Ÿæˆ ---
from google.genai import types


def query_rag_system(user_query: str, category_filter: str) -> tuple[str, list]:
    # 1. æ£€ç´¢ (Retrieval)
    # æ ¹æ®ç”¨æˆ·é€‰æ‹©çš„æ¨¡å¼ï¼ŒåŠ¨æ€æ„é€  where è¿‡æ»¤æ¡ä»¶
    where_condition = {"category": category_filter}

    results = collection.query(
        # å‡è®¾åªæœ‰ä¸€ä¸ªé—®é¢˜
        query_texts=[user_query],
        n_results=3,  # æ‰¾ 3 æ¡è¯æ®
        where=where_condition,  # ğŸ”¥ Day 10 çš„æ ¸å¿ƒé­”æ³•
    )

    # 2. ç»„è£… Context
    valid_docs = results["documents"][0]
    metadatas = results["metadatas"][0]

    if not valid_docs:
        return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³èµ„æ–™ã€‚", []

    context_str = "\n".join(valid_docs)

    # 3. ç”Ÿæˆ (Generation)
    system_prompt = """
    ###è§’è‰²
    ä½ æ˜¯ä¸€ä¸ªåŸºäºã€ç§æœ‰çŸ¥è¯†åº“ã€‘çš„æ™ºèƒ½åŠ©æ‰‹ã€‚
    ###ä»»åŠ¡
    è¯·æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
    ###çº¦æŸ
    1.å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç­”æ¡ˆï¼Œè¯·è¯šå®åœ°è¯´ä¸çŸ¥é“ï¼Œä¸è¦ç¼–é€ ã€‚
    2.åŒºåˆ†â€œäº‹ä»¶çš„æ¬¡æ•°â€å’Œâ€œåŠ¨ä½œçš„é¢‘ç‡â€ã€‚ä¾‹å¦‚ï¼Œâ€œæ‰“äº†ä¸‰æ¬¡çƒâ€é€šå¸¸æ„å‘³ç€â€œè¿™æ˜¯ä¸€æ¬¡æ‰“çƒçš„æ´»åŠ¨â€ã€‚
    3.å¦‚æœèµ„æ–™ä¸­æ²¡æœ‰æ˜ç¡®æåˆ°è§é¢çš„æ¬¡æ•°ï¼Œè¯·å›ç­”â€œèµ„æ–™ä¸­åªæåˆ°äº†å…·ä½“çš„äº’åŠ¨ç»†èŠ‚ï¼Œæœªç»Ÿè®¡è§é¢æ€»æ¬¡æ•°â€ã€‚
    """

    user_prompt = f"""
    ã€å‚è€ƒèµ„æ–™ ({category_filter}ç±»)ã€‘
    {context_str}
    
    ã€ç”¨æˆ·é—®é¢˜ã€‘
    {user_query}
    """

    try:
        response = gemini_client.models.generate_content(
            model="gemini-2.0-flash-exp",
            config=types.GenerateContentConfig(
                system_instruction=system_prompt,
                temperature=0.3,
            ),
            contents=[user_prompt],
        )
        return response.text, metadatas
    except Exception as e:
        return f"âŒ è°ƒç”¨ Gemini å‡ºé”™: {str(e)}", []


# --- 3. Streamlit UI ç•Œé¢æ„å»º ---


st.set_page_config(page_title="èƒ–å¢©å¢©çš„çŸ¥è¯†åº“", page_icon="ğŸ¶", layout="wide")

# === ä¾§è¾¹æ ï¼šæ§åˆ¶å° ===
with st.sidebar:
    st.image(
        "img/DSC01879.jpeg", caption="æˆ‘æ˜¯èƒ–å¢©å¢©", use_container_width=True
    )  # å‡è®¾ä½ ä¸Šä¼ äº†è¿™å¼ å›¾
    st.title("ğŸ›ï¸ æ¨¡å¼é€‰æ‹©")

    # æ ¸å¿ƒäº¤äº’æ§ä»¶ï¼šé€‰æ‹©æŸ¥è¯¢èŒƒå›´
    mode = st.radio("ä½ æƒ³æŸ¥è¯¢ä»€ä¹ˆï¼Ÿ", ("ğŸ¶ èƒ–å¢©å¢©çš„ç”Ÿæ´»æ—¥è®°", "ğŸ’» èµµä¸€æ¸…çš„æŠ€æœ¯æ–‡æ¡£"))

    # æ˜ å°„åˆ°æ•°æ®åº“çš„ category æ ‡ç­¾
    if "æ—¥è®°" in mode:
        category_filter = "diary"
        st.info("å½“å‰æ¨¡å¼ï¼šåªæ£€ç´¢ã€æ—¥è®°ã€‘JSON æ•°æ®")
    else:
        category_filter = "tech"
        st.success("å½“å‰æ¨¡å¼ï¼šåªæ£€ç´¢ã€æŠ€æœ¯ã€‘Markdown æ–‡æ¡£")

    st.markdown("---")
    st.caption("Powered by zhao yi qing")

# === ä¸»ç•Œé¢ï¼šèŠå¤©çª—å£ ===
st.title("ğŸ’¬ ç§æœ‰çŸ¥è¯†åº“åŠ©æ‰‹")

# åˆå§‹åŒ–èŠå¤©å†å² (Session State)
if "messages" not in st.session_state:
    st.session_state.messages = []

# 1. æ¸²æŸ“å†å²æ¶ˆæ¯
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# 2. æ¥æ”¶ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜..."):
    # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # 3. è°ƒç”¨ AI (æ˜¾ç¤ºåŠ è½½åŠ¨ç”»)
    with st.chat_message("assistant"):
        with st.spinner(f"ğŸ” æ­£åœ¨æ£€ç´¢ {category_filter} åº“..."):
            answer, sources = query_rag_system(prompt, category_filter)

            # æ˜¾ç¤ºå›ç­”
            st.markdown(answer)

            # æ˜¾ç¤ºå¼•ç”¨æ¥æº (æŠ˜å çŠ¶æ€)
            if sources:
                with st.expander("ğŸ“š æŸ¥çœ‹å‚è€ƒæ¥æº (Evidence)"):
                    for i, meta in enumerate(sources):
                        # åŠ¨æ€å±•ç¤º Metadata
                        source_name = meta.get("source", "æœªçŸ¥æ–‡ä»¶")
                        author = meta.get("author", meta.get("subject", "æœªçŸ¥"))
                        st.caption(
                            f"ğŸ“„ æ¥æº {i+1}: **{source_name}** (ç›¸å…³äºº: {author})"
                        )

    # ä¿å­˜ AI å›å¤åˆ°å†å²
    st.session_state.messages.append({"role": "assistant", "content": answer})
