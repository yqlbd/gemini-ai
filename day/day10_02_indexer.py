import os
import glob
import json
import chromadb
from chromadb import EmbeddingFunction, Documents, Embeddings
from sentence_transformers import SentenceTransformer


# --- 1. å¤ç”¨ Embedding é…ç½® ---
class LocalEmbeddingFunction(EmbeddingFunction):
    def __init__(self):
        # æ—¢ç„¶æ˜¯å¤„ç†æ–‡æ¡£ï¼Œæˆ‘ä»¬ç”¨è¿™ä¸€æ¬¾æ”¯æŒå¤šè¯­è¨€çš„æ¨¡å‹ï¼Œæ•ˆæœæ¯”è¾ƒå‡è¡¡
        self.model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

    def __call__(self, input: Documents) -> Embeddings:
        embeddings = self.model.encode(input)
        # åˆ—è¡¨æ¨å¯¼å¼ï¼ŒæŠŠæ¯ä¸ª numpy array è½¬æˆ list
        return [e.tolist() for e in embeddings]

    def name(self):
        return "local_sentence_transformer_v2"


# è¿æ¥æ•°æ®åº“ï¼Œå®šä¹‰å¥½æ•°æ®åº“åœ°å€ï¼Œè¿™é‡Œä¸ºrst/chroma_db
print("ğŸ’¾ æ­£åœ¨è¿æ¥è®°å¿†åº“...")
client = chromadb.PersistentClient(path="rst/chroma_db")

# âš ï¸ æ³¨æ„ï¼šä¸ºäº†æ¼”ç¤ºæ•ˆæœï¼Œæˆ‘ä»¬è¿™æ¬¡åˆ›å»ºä¸€ä¸ªå…¨æ–°çš„é›†åˆï¼Œå« "categorized_memory" (åˆ†ç±»è®°å¿†)ï¼Œç›¸å½“äºè¡¨å
# è¿™æ ·ä¸ä¼šå’Œä¹‹å‰çš„æ··ä¹±æ•°æ®æ··åœ¨ä¸€èµ·
collection = client.get_or_create_collection(
    name="categorized_memory", embedding_function=LocalEmbeddingFunction()
)


# --- 2. æ ¸å¿ƒå·¥å…·ï¼šåˆ‡ç‰‡å™¨ ---
"""
text: å¾…åˆ‡ç‰‡çš„æ–‡æœ¬
chunk_size: æ¯ä¸ªåˆ‡ç‰‡çš„æœ€å¤§é•¿åº¦ï¼Œé»˜è®¤300å­—ç¬¦
chunk_overlap: åˆ‡ç‰‡ä¹‹é—´é‡å çš„éƒ¨åˆ†ï¼Œä¿è¯ä¸Šä¸‹æ–‡è¿è´¯ï¼Œé»˜è®¤50å­—ç¬¦
"""


def split_text(text, chunk_size=300, chunk_overlap=50) -> list:
    """æ»‘åŠ¨çª—å£åˆ‡ç‰‡ï¼šä¿è¯ä¸Šä¸‹æ–‡è¿è´¯"""
    chunks = []
    # æ¸¸æ ‡ä½ç½®
    start = 0
    # åˆ‡ç‰‡æ–‡æœ¬é•¿åº¦
    text_len = len(text)

    # å½“æ¸¸æ ‡æ²¡åˆ°æ–‡æœ¬æœ«å°¾æ—¶ï¼Œå¾ªç¯æŒç»­åˆ‡ç‰‡
    while start < text_len:
        # åˆ‡ç‰‡ç»“æŸä½ç½®
        end = start + chunk_size
        # åˆ‡ç‰‡æ–‡æœ¬ï¼Œå–æ¸¸æ ‡åˆ°ç»“æŸä½ç½®ä¹‹é—´çš„å†…å®¹
        chunk = text[start:end]
        # åŠ å…¥é›†åˆ
        chunks.append(chunk)
        # æ­¥é•¿ = çª—å£å¤§å° - é‡å éƒ¨åˆ†
        start += chunk_size - chunk_overlap
    return chunks


# --- 3. å¤„ç†ä¸åŒç±»å‹çš„æ–‡ä»¶ ---
# å¤„ç†*.md æŠ€æœ¯æ–‡æ¡£ï¼Œæ‰“ä¸Š category: tech æ ‡ç­¾
def process_tech_docs(directory) -> None:
    """å¤„ç†æŠ€æœ¯æ–‡æ¡£ (.md) -> æ‰“ä¸Š category: tech"""
    files = glob.glob(os.path.join(directory, "*.md"))
    print(f"\nğŸ“˜ å‘ç° {len(files)} ä¸ªæŠ€æœ¯æ–‡æ¡£")

    for file_path in files:
        filename = os.path.basename(file_path)
        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()

        # åˆ‡ç‰‡ï¼Œæ¯ç‰‡400å­—ç¬¦ï¼Œé‡å 50å­—ç¬¦
        chunks = split_text(content, chunk_size=400, chunk_overlap=50)

        # å‡†å¤‡å…¥åº“æ•°æ®
        ids = [f"tech_{filename}_{i}" for i in range(len(chunks))]

        # ğŸ”¥ å…³é”®æ­¥éª¤ï¼šæ‰“æ ‡ç­¾ï¼
        # æˆ‘ä»¬æ˜ç¡®æŒ‡å®šè¿™æ˜¯ "tech" ç±»ï¼Œä½œè€…æ˜¯ "èµµä¸€æ¸…"
        # åˆ—è¡¨æ¨å¯¼å¼ç”Ÿæˆ metadatas
        metadatas = [
            {
                "category": "tech",
                "author": "èµµä¸€æ¸…",
                "source": filename,
                "chunk_index": i,
            }
            for i in range(len(chunks))
        ]

        print(f"   â†³ æ­£åœ¨å­˜å…¥ '{filename}': {len(chunks)} ä¸ªç¢ç‰‡ (Tag: tech)")
        # å…¥åº“ï¼Œæ¯æ¬¡è¯»åˆ°.md æ–‡ä»¶å°±å­˜ä¸€æ‰¹
        collection.upsert(ids=ids, documents=chunks, metadatas=metadatas)


# å¤„ç†*.json æ—¥è®°æ–‡ä»¶ï¼Œæ‰“ä¸Š category: diary æ ‡ç­¾
def process_diary_logs(directory):
    """å¤„ç†æ—¥è®°æ–‡ä»¶ (.json) -> æ‰“ä¸Š category: diary"""
    files = glob.glob(os.path.join(directory, "*.json"))
    print(f"\nCb å‘ç° {len(files)} ä¸ªæ—¥è®°æ–‡ä»¶")

    for file_path in files:
        filename = os.path.basename(file_path)
        with open(file_path, "r", encoding="utf-8") as f:
            try:
                data = json.load(f)  # å‡è®¾æ˜¯ list of dict
                # åˆ¤æ–­æ˜¯åˆ—è¡¨è¿˜æ˜¯å­—å…¸ï¼Œå¦‚æœæ˜¯[]å°±ç»§ç»­ï¼ŒåŒç†ï¼Œ{}æ˜¯dict
                if not isinstance(data, list):
                    print(f"   âš ï¸ è·³è¿‡ {filename}: æ ¼å¼ä¸æ˜¯åˆ—è¡¨")
                    continue
            except:
                print(f"   âš ï¸ è·³è¿‡ {filename}: JSON è§£æå¤±è´¥")
                continue

        chunks = []
        ids = []
        metadatas = []

        # åŒæ—¶è·å–è§’æ ‡å’Œå¯¹è±¡çš„å†™æ³•ï¼Œä»0å¼€å§‹ï¼Œentryæ˜¯ä¸ªdictå¯¹è±¡ï¼Œç›´æ¥å¯ä»¥getå­—æ®µ
        for i, entry in enumerate(data):
            # æŠŠ JSON å¯¹è±¡è½¬æˆè¿™ç§æ˜“è¯»çš„å­—ç¬¦ä¸²
            # å‡è®¾ entry é•¿è¿™æ ·: {"timestamp": "...", "event": "..."}
            text_chunk = f"æ—¶é—´: {entry.get('timestamp', 'æœªçŸ¥')}\näº‹ä»¶: {entry.get('event', str(entry))}"

            chunks.append(text_chunk)
            ids.append(f"diary_{filename}_{i}")

            # ğŸ”¥ å…³é”®æ­¥éª¤ï¼šæ‰“æ ‡ç­¾ï¼
            # æ˜ç¡®è¿™æ˜¯ "diary" ç±»ï¼Œä¸»è§’æ˜¯ "èƒ–å¢©å¢©"
            metadatas.append(
                {"category": "diary", "subject": "èƒ–å¢©å¢©", "source": filename}
            )

        if chunks:
            print(f"   â†³ æ­£åœ¨å­˜å…¥ '{filename}': {len(chunks)} æ¡è®°å½• (Tag: diary)")
            collection.upsert(ids=ids, documents=chunks, metadatas=metadatas)


# --- è¿è¡Œä¸»ç¨‹åº ---
if __name__ == "__main__":
    target_dir = "./rst"  # ä½ çš„æ–‡ä»¶éƒ½åœ¨è¿™é‡Œ

    # 1. æ¸…ç©ºæ—§æ•°æ® (ä¸ºäº†æ¼”ç¤ºçº¯å‡€çš„æ•ˆæœ)
    # collection.delete(where={}) # å¦‚æœä½ æƒ³è¿½åŠ è€Œä¸æ˜¯è¦†ç›–ï¼Œå°±æŠŠè¿™è¡Œæ³¨é‡Šæ‰

    # 2. åˆ†ç±»å¤„ç†
    process_tech_docs(target_dir)
    process_diary_logs(target_dir)

    print("\nâœ… ç´¢å¼•é‡å»ºå®Œæˆï¼æ•°æ®å·²æ‰“ä¸Š Metadata æ ‡ç­¾ã€‚")
    print("ç°åœ¨ Gemini ä¸ä¼šå†æŠŠèƒ–å¢©å¢©å½“æˆæ¶æ„å¸ˆäº†ï¼ğŸ¶")
