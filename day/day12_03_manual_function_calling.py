"""
LLMè‡ªåŠ¨è°ƒç”¨function calling
"""

from google import genai
from google.genai import types
import os
import asyncio


# è‡ªå®šä¹‰çš„apiï¼Œæ¨¡æ‹Ÿè·å–å¤©æ°”æ•°æ®
def get_weather_info(city: str = "ä¸Šæµ·") -> str:
    mock_data = {"ä¸Šæµ·": "18åº¦ï¼Œæ™´è½¬å¤šäº‘", "åŒ—äº¬": "20åº¦ï¼Œæ™´", "å¹¿å·": "16åº¦ï¼Œé˜´è½¬å°é›¨"}
    return mock_data.get(city, "æœªæ‰¾åˆ°å½“åœ°å¤©æ°”ï¼Œè¯·è”ç³»æ°”è±¡éƒ¨é—¨")


def get_name_info(role: str = "å°ç‹—") -> str:
    mock_data = {"å¦ˆå¦ˆ": "éœå¦®åª›", "å°ç‹—": "èƒ–å¢©å¢©", "æˆ‘": "èµµä¸€æ¸…"}
    return mock_data.get(role, "æœªæ‰¾åˆ°å¯¹åº”ä¿¡æ¯")


model_id = "gemini-2.0-flash"

tool_list = [get_weather_info, get_name_info]
client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))
FUNCTION_MAP = {"get_weather_info": get_weather_info, "get_name_info": get_name_info}


# è°ƒç”¨
async def get_gemini_response(question: str) -> None:
    chat_history = []
    if question:
        chat_history.append(
            types.Content(role="user", parts=[types.Part.from_text(text=question)])
        )
        print(f"ğŸ‘¨çš„æé—®ï¼š{question}")
        response = await client.aio.models.generate_content(
            model=model_id,
            contents=chat_history,
            config=types.GenerateContentConfig(
                system_instruction="ä½ æ˜¯ä¸ªAIåŠ©ç†ï¼Œå¦‚æœè¯¢é—®åå­—ï¼Œè¯·è°ƒç”¨fucntion_calling",
                temperature=0.3,
                tools=tool_list,
                # å¢åŠ è¿™ä¸ªé…ç½®ï¼Œç”¨æ¥æ‰‹åŠ¨è°ƒç”¨
                automatic_function_calling={"disable": True},
            ),
        )
        candidates = response.candidates[0]
        # é˜²å¾¡æ€§ç¼–ç¨‹ï¼šæœ‰æ—¶å€™ Gemini å¯èƒ½ä¼šè¿”å›ç©ºå†…å®¹
        if not candidates.content or not candidates.content.parts:
            print("âŒ é”™è¯¯ï¼šGemini è¿”å›äº†ç©ºå†…å®¹")
            return

        for part in candidates.content.parts:
            # å¦‚æœæ˜¯function_calling
            if part.function_call:
                fn_name = part.function_call.name
                fn_args = part.function_call.args
                print(f"ğŸ¤– [åŠ¨æ€åˆ†å‘] è°ƒç”¨æ–¹æ³•: {fn_name} | å‚æ•°: {fn_args}")
                if fn_name in FUNCTION_MAP:
                    target_function = FUNCTION_MAP[fn_name]
                    try:
                        # â­ é­”æ³•æ—¶åˆ»ï¼š**fn_args æ˜¯ Python çš„å‚æ•°è§£åŒ…
                        # åªè¦ Gemini ç»™çš„å‚æ•°å(key)å’Œå‡½æ•°å®šä¹‰çš„å‚æ•°åä¸€è‡´ï¼Œå®ƒå°±ä¼šè‡ªåŠ¨å¡«å…¥
                        tool_result = target_function(**fn_args)
                        print(f"ğŸ“¦ [æ‰§è¡ŒæˆåŠŸ] ç»“æœ: {tool_result}")
                    except Exception as e:
                        tool_result = f"å‡½æ•°æ‰§è¡Œå‡ºé”™: {str(e)}"
                        print(f"âŒ [æ‰§è¡Œå¤±è´¥] {str(e)}")
                else:
                    tool_result = f"é”™è¯¯ï¼šæœªçŸ¥çš„å·¥å…· {fn_name}"
                    print(f"âš ï¸ [æœªçŸ¥å·¥å…·] {fn_name}")

                function_response_part = types.Part.from_function_response(
                    name=fn_name, response={"result": tool_result}
                )

                # æ›´æ–°å†å²å¹¶å‘é€
                # æ‹¼è£…modelçš„functioncallä¿¡æ¯
                chat_history.append(candidates.content)
                # æ‹¼è£…toolè¿”å›çš„ä¿¡æ¯
                chat_history.append(
                    types.Content(role="tool", parts=[function_response_part])
                )
                # æœ€åçš„æ•°æ®ç¤ºä¾‹ï¼š[{'role':'user',[{'text':'å¹¿å·å¤©æ°”å¦‚ä½•',...}]},{'role':''model',[{'funtion call':...}]},{'role':'tool',...}]
                final_response = await client.aio.models.generate_content(
                    model=model_id,
                    contents=chat_history,
                    config=types.GenerateContentConfig(temperature=2.0),
                )
                print(f"ğŸ¤–çš„å›å¤: {final_response.text}")
                return

        if response.text:
            print(f"ğŸ¤–çš„å›å¤ï¼š{response.text}")


async def main():
    # æ‰€æœ‰çš„ä¸šåŠ¡é€»è¾‘éƒ½æ”¾åœ¨è¿™ä¸ª main å¼‚æ­¥å‡½æ•°é‡Œ
    question = "ä½ æ˜¯ä»€ä¹ˆæ¨¡å‹ï¼Ÿ"
    await get_gemini_response(question=question)

    question2 = "å¹¿å·å¤©æ°”å¦‚ä½•ï¼Ÿ"
    await get_gemini_response(question=question2)

    question3 = "å°ç‹—å«å•¥åå­—ï¼Ÿ"
    await get_gemini_response(question=question3)


if __name__ == "__main__":
    asyncio.run(main())
