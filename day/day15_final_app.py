# æ–‡ä»¶è·¯å¾„: day/day15_final_app.py

import streamlit as st
import os
import sys
import time
from dotenv import load_dotenv
from google import genai
from google.genai import types

# ç¡®ä¿èƒ½å¼•ç”¨åˆ° utils
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from day.utils.ai_tools import (
    tools_list,
    get_current_weather,
    calculate_dog_food,
    search_knowledge_base,
)

# 1. åˆå§‹åŒ–ç¯å¢ƒ
load_dotenv()
st.set_page_config(page_title="èƒ–å¢©å¢©å…¨èƒ½ç®¡å®¶", page_icon="ğŸ¶", layout="centered")

# --- æ ¸å¿ƒï¼šå®šä¹‰æ”¯æŒ UI åé¦ˆçš„ Agent ---
# æˆ‘ä»¬æŠŠ Day 14 çš„é€»è¾‘æ¬è¿‡æ¥ï¼Œå¹¶åŠ ä¸Š Streamlit çš„è§†è§‰åé¦ˆ
FUNCTION_MAP = {
    "get_current_weather": get_current_weather,
    "calculate_dog_food": calculate_dog_food,
    "search_knowledge_base": search_knowledge_base,
}


class StreamlitAgent:
    def __init__(self, model_id="gemini-2.0-flash-exp"):
        self.client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))
        self.model_id = model_id
        self.chat_history = []

        # Day 14 çš„ç»ˆæ Prompt
        self.system_instruction = """
        ä½ æ˜¯ä¸€ä¸ªå…¨èƒ½å‹æ™ºèƒ½åŠ©æ‰‹ï¼Œåå­—å«â€œèƒ–å¢©å¢©ç®¡å®¶â€ã€‚
        ä½ æ‹¥æœ‰ä»¥ä¸‹å¼ºåŠ›å·¥å…·ï¼š
        1. `search_knowledge_base`: **æ ¸å¿ƒå·¥å…·**ã€‚å½“é—®é¢˜æ¶‰åŠâ€œæˆ‘â€ã€â€œèƒ–å¢©å¢©â€ã€â€œæ—¥è®°â€ã€â€œä»¥å‰â€æˆ–â€œç¬”è®°â€ç­‰ç§æœ‰ä¿¡æ¯æ—¶ï¼Œ**å¿…é¡»ä¼˜å…ˆè°ƒç”¨**æ­¤å·¥å…·æŸ¥åº“ã€‚
        2. `get_current_weather`: æŸ¥è¯¢å®æ—¶å¤©æ°”ã€‚
        3. `calculate_dog_food`: è®¡ç®—ç‹—ç²®ç”¨é‡ã€‚

        æ€è€ƒä¸è¡ŒåŠ¨ç­–ç•¥ (ReAct Loop):
        - æ”¶åˆ°é—®é¢˜åï¼Œå…ˆåˆ†æéœ€è¦å“ªäº›ä¿¡æ¯ã€‚
        - é‡åˆ°ç§æœ‰çŸ¥è¯†ï¼Œè°ƒ `search_knowledge_base`ã€‚
        - é‡åˆ°å®¢è§‚äº‹å®ï¼Œè°ƒ `get_current_weather`ã€‚
        - æ‹¿åˆ°å·¥å…·ç»“æœåï¼Œç»“åˆä½ çš„å¸¸è¯†è¿›è¡Œç»¼åˆå›ç­”ã€‚
        """

    def chat(self, user_query):
        # æŠŠç”¨æˆ·é—®é¢˜åŠ å…¥å†å²
        self.chat_history.append(
            types.Content(role="user", parts=[types.Part.from_text(text=user_query)])
        )

        max_turns = 5
        turn_count = 0

        # åœ¨ UI ä¸Šæ˜¾ç¤ºä¸€ä¸ªçŠ¶æ€å®¹å™¨
        with st.status("ğŸ§  å¤§è„‘é£é€Ÿè¿è½¬ä¸­...", expanded=True) as status:

            while turn_count < max_turns:
                turn_count += 1
                st.write(f"ğŸ”„ ç¬¬ {turn_count} è½®æ€è€ƒ...")

                # è°ƒç”¨ Gemini
                response = self.client.models.generate_content(
                    model=self.model_id,
                    contents=self.chat_history,
                    config=types.GenerateContentConfig(
                        tools=tools_list,
                        temperature=0.0,
                        system_instruction=self.system_instruction,
                        automatic_function_calling={"disable": True},
                    ),
                )

                # 1. æ£€æŸ¥å·¥å…·è°ƒç”¨
                if self._has_function_call(response):
                    # åœ¨ UI ä¸Šå‘ŠçŸ¥ç”¨æˆ·æ­£åœ¨è°ƒç”¨å·¥å…·
                    call_names = [
                        part.function_call.name
                        for part in response.candidates[0].content.parts
                        if part.function_call
                    ]
                    st.info(f"ğŸ› ï¸ æ­£åœ¨è°ƒç”¨å·¥å…·: {', '.join(call_names)}")

                    self.chat_history.append(response.candidates[0].content)
                    self._execute_tool_calls(response.candidates[0].content.parts)
                    continue

                # 2. æ£€æŸ¥æ–‡æœ¬å›ç­”
                if response.text:
                    status.update(
                        label="âœ… æ€è€ƒå®Œæˆï¼", state="complete", expanded=False
                    )
                    self.chat_history.append(response.candidates[0].content)
                    return response.text

                break

            status.update(label="âš ï¸ æ€è€ƒè¶…æ—¶æˆ–ä¸­æ–­", state="error")
            return "æŠ±æ­‰ï¼Œæˆ‘æ€è€ƒå¤ªä¹…äº†ï¼Œæœ‰ç‚¹ä¹±..."

    def _has_function_call(self, response):
        if not response.candidates:
            return False
        for part in response.candidates[0].content.parts:
            if part.function_call:
                return True
        return False

    def _execute_tool_calls(self, parts):
        response_parts = []
        for part in parts:
            if part.function_call:
                fn_name = part.function_call.name
                fn_args = part.function_call.args

                # UI åé¦ˆï¼šå¦‚æœæ˜¯æŸ¥åº“ï¼Œæ˜¾ç¤ºä¸ªç‰¹åˆ«çš„ Toast
                if fn_name == "search_knowledge_base":
                    st.toast(f"ğŸ“š æ­£åœ¨ç¿»é˜…æ—¥è®°åº“...", icon="ğŸ“–")
                elif fn_name == "get_current_weather":
                    st.toast(f"â˜ï¸ æ­£åœ¨æŸ¥è¯¢å¤©æ°”...", icon="ğŸŒ¦ï¸")

                if fn_name in FUNCTION_MAP:
                    try:
                        result = FUNCTION_MAP[fn_name](**fn_args)
                        # åœ¨çŠ¶æ€æ é‡ŒæŠ˜å æ˜¾ç¤ºè¯¦ç»†ç»“æœï¼Œé¿å…åˆ·å±
                        with st.expander(f"ğŸ“¦ å·¥å…· {fn_name} è¿”å›ç»“æœ"):
                            st.code(str(result)[:500])  # åªæ˜¾ç¤ºå‰500å­—
                    except Exception as e:
                        result = f"Error: {e}"
                else:
                    result = f"Error: Unknown tool {fn_name}"

                response_parts.append(
                    types.Part.from_function_response(
                        name=fn_name, response={"result": result}
                    )
                )

        if response_parts:
            self.chat_history.append(types.Content(role="tool", parts=response_parts))


# --- UI ä¸»ç¨‹åºé€»è¾‘ ---

st.title("ğŸ¶ èƒ–å¢©å¢©å…¨èƒ½ç®¡å®¶")
st.caption("ğŸš€ æ¶æ„: Streamlit + Gemini 2.0 + RAG Memory + Tools")

# ä¾§è¾¹æ 
with st.sidebar:
    st.image(
        "img/DSC01879.jpeg", caption="æˆ‘æ˜¯èƒ–å¢©å¢©", use_container_width=True
    )  # å‡è®¾ä½ ä¸Šä¼ äº†è¿™å¼ å›¾)
    st.header("åŠŸèƒ½å±•ç¤º")
    st.markdown(
        """
    è¯•ç€é—®æˆ‘ï¼š
    1. **æŸ¥å¤©æ°”**: å¸¸å·ä»Šå¤©å¤©æ°”å’‹æ ·ï¼Ÿ
    2. **æŸ¥ç®—æœ¯**: 8.5kg ç‹—åƒå¤šå°‘ï¼Ÿ
    3. **æŸ¥è®°å¿† (RAG)**: èƒ–å¢©å¢©ä»¥å‰ç©è¿‡é£ç›˜å—ï¼Ÿ
    4. **æ··åˆåŒæ‰“**: 
       > *ç»“åˆèƒ–å¢©å¢©çš„èº«ä½“æƒ…å†µï¼ˆæŸ¥æ—¥è®°ï¼‰ï¼Œçœ‹çœ‹ä»Šå¤©å¸¸å·çš„å¤©æ°”é€‚åˆå¸¦å®ƒå»æˆ·å¤–ç©å—ï¼Ÿ*
    """
    )
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯è®°å¿†"):
        st.session_state.messages = []
        st.session_state.agent = StreamlitAgent()  # é‡ç½® Agent
        st.rerun()

# åˆå§‹åŒ– Session
if "messages" not in st.session_state:
    st.session_state.messages = []
if "agent" not in st.session_state:
    st.session_state.agent = StreamlitAgent()

# æ¸²æŸ“å†å²æ¶ˆæ¯
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# å¤„ç†è¾“å…¥
if prompt := st.chat_input("å¬å”¤å…¨èƒ½ç®¡å®¶..."):
    # 1. æ˜¾ç¤ºç”¨æˆ·è¾“å…¥
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # 2. Agent å¼€å§‹è¡¨æ¼”
    with st.chat_message("assistant"):
        # ç›´æ¥è°ƒç”¨ Agentï¼Œå†…éƒ¨ä¼šå¤„ç† UI çŠ¶æ€æ˜¾ç¤º
        response_text = st.session_state.agent.chat(prompt)

        # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
        st.markdown(response_text)
        st.session_state.messages.append(
            {"role": "assistant", "content": response_text}
        )
